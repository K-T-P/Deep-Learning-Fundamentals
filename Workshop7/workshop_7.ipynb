{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "88072146",
      "metadata": {
        "id": "88072146"
      },
      "source": [
        "# Language Models and Parameter-Efficient Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37e65237",
      "metadata": {
        "id": "37e65237"
      },
      "source": [
        "## Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54863f40",
      "metadata": {
        "id": "54863f40"
      },
      "source": [
        "\n",
        "Language models are fundamental to natural language processing. They come in three major categories:\n",
        "\n",
        "1. **Encoder-only models**: (e.g., BERT, RoBERTa, ELECTRA) - Best suited for understanding tasks such as classification and regression.\n",
        "2. **Encoder-decoder models**: (e.g., T5, BART) - Ideal for tasks like translation and summarization.\n",
        "3. **Decoder-only models**: (e.g., GPT-n models) - Primarily used for text generation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a11efa3",
      "metadata": {
        "id": "3a11efa3"
      },
      "source": [
        "## Autoregressive Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31880454",
      "metadata": {
        "id": "31880454"
      },
      "source": [
        "\n",
        "Autoregressive models predict the next token in a sequence based on previous tokens. This enables **conditional generation**, where outputs depend on the given prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aXfsHcaJR_ll",
      "metadata": {
        "id": "aXfsHcaJR_ll"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NuNYRTyHQ2VM",
      "metadata": {
        "id": "NuNYRTyHQ2VM"
      },
      "outputs": [],
      "source": [
        "## Now let's use old - GPT2 for generating text\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Initialize GPT-2 model and tokenizer\n",
        "def initialize_gpt2():\n",
        "    tokenizer = ## TODO use pretrained tokenizer of gpt2\n",
        "    model = ## TODO use pretrained model of gpt2\n",
        "    return tokenizer, model\n",
        "\n",
        "gpt_tokenizer, gpt_model = initialize_gpt2()\n",
        "\n",
        "# Function to generate text using GPT-2\n",
        "def generate_text(prompt, max_length=50, temperature=1.0, top_k=50):\n",
        "    \"\"\"\n",
        "    Generate text using GPT-2 with customizable parameters.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The initial text to seed the model.\n",
        "        max_length (int): The maximum length of the generated text.\n",
        "        temperature (float): Sampling temperature. Higher values make output more random.\n",
        "        top_k (int): Limits sampling to the top-k most likely tokens.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated text.\n",
        "    \"\"\"\n",
        "    input_ids = gpt_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate text\n",
        "    outputs = gpt_model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return ## TODO return the decoded generated message\n",
        "\n",
        "prompts = [\n",
        "## TODO input some arbitary prompts,\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"Input Prompt: {prompt}\")\n",
        "    print(f\"Generated Text: {generate_text(prompt, max_length=100, temperature=0.7)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "682ef732",
      "metadata": {
        "id": "682ef732"
      },
      "source": [
        "## Large Language Models (LLMs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20f6d9e9",
      "metadata": {
        "id": "20f6d9e9"
      },
      "source": [
        "\n",
        "Large Language Models (LLMs) scale up the size and capacity of traditional language models. Key concepts include:\n",
        "\n",
        "- **Scale**: Models like GPT-3 have billions of parameters, leading to significant improvements in performance.\n",
        "- **Pre-training and Adaptation**: Pre-trained on massive datasets and later adapted to specific tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "264fa2d8",
      "metadata": {
        "id": "264fa2d8"
      },
      "source": [
        "[link text](https://)## Ways to Adapt to New Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3454d3d",
      "metadata": {
        "id": "e3454d3d"
      },
      "source": [
        "\n",
        "Methods to adapt pre-trained models include:\n",
        "\n",
        "1. **Zero-shot learning**: Use task descriptions as prompts without any training examples.\n",
        "2. **Few-shot learning**: Provide a small number of task-specific examples.\n",
        "3. **Lightweight Fine-tuning**: Modify only a subset of the model's parameters.\n",
        "4. **Fine-tuning for human-aligned models**: Align models with human preferences using fine-tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b1ff5a6",
      "metadata": {
        "id": "7b1ff5a6"
      },
      "source": [
        "## Zero-shot Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bb64bea",
      "metadata": {
        "id": "6bb64bea"
      },
      "source": [
        "\n",
        " Fine-tuning T5 on a multi-task dataset for zero-shot learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ck3JJrmzZCDe",
      "metadata": {
        "id": "Ck3JJrmzZCDe"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-yxyTiAgifkw",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-yxyTiAgifkw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "def zero_shot_question_answering(context, question, model_name=\"bigscience/T0_3B\"):\n",
        "    # Load the tokenizer and model from Hugging Face Hub\n",
        "    tokenizer = ## TODO use pretrained tokenizer\n",
        "    model = ## TODO use pretrained model\n",
        "\n",
        "\n",
        "    # Prepare the input with the context and question\n",
        "    prompt = f\"context: {context} question: {question}\"\n",
        "\n",
        "    # Tokenize the prompt , with padding and truncation,\n",
        "    inputs = ## TODO\n",
        "\n",
        "    # Generate an answer from the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(inputs[\"input_ids\"], max_length=50, num_beams=5)\n",
        "\n",
        "    # Decode the generated output to get the answer\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Define the context and the question\n",
        "context = ## TODO\n",
        "question = ## TODO\n",
        "\n",
        "# Perform question answering\n",
        "answer = zero_shot_question_answering(context, question)\n",
        "print(\"Answer:\", answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ada95b24",
      "metadata": {
        "id": "ada95b24"
      },
      "source": [
        "## Few-shot Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45063e96",
      "metadata": {
        "id": "45063e96"
      },
      "source": [
        "\n",
        "Few-shot learning strategies include:\n",
        "\n",
        "1. **Prompt-based fine-tuning**: Modify the prompt to improve performance.\n",
        "2. **In-context learning (ICL)**: Provide a few examples as part of the prompt for task demonstrations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZqbEPEXTfmrc",
      "metadata": {
        "id": "ZqbEPEXTfmrc"
      },
      "source": [
        "#### Prompt-based fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdjj0yw-fp0S",
      "metadata": {
        "id": "cdjj0yw-fp0S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from torch.optim import AdamW  # Use PyTorch's AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define a custom dataset for prompt-based fine-tuning\n",
        "class PromptDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prompt, response = self.data[idx]\n",
        "        encoded_input = self.tokenizer(\n",
        "            prompt,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        encoded_response = self.tokenizer(\n",
        "            response,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoded_input[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoded_input[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": encoded_response[\"input_ids\"].squeeze(0),\n",
        "        }\n",
        "\n",
        "# Load pre-trained model and tokenizer of t5-small\n",
        "tokenizer = ## TODO\n",
        "model = ## TODO\n",
        "\n",
        "# Load SST-2 dataset\n",
        "raw_dataset = load_dataset(\"glue\", ## TODO) ## use a dataset for sentiment analysis task like sst-2 or sst-5\n",
        "\n",
        "# Prepare data for fine-tuning\n",
        "processed_data = []\n",
        "for example in raw_dataset[\"train\"]:\n",
        "    sentence = ## TODO sentence of your each example in raw_dataset_train\n",
        "    label = ## TODO define the label based on your dataset\n",
        "    prompt = f\"Sentiment analysis: {sentence} The sentiment is [MASK].\"\n",
        "    response = ## TODO\n",
        "    processed_data.append((prompt, response))\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = PromptDataset(processed_data, tokenizer, max_length=64)\n",
        "dataloader = ## TODO use the Dataloader with arbitary batch size.for example 16\n",
        "\n",
        "# Define optimizer and device\n",
        "optimizer = ## TODO  # Use PyTorch's AdamW\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Fine-tuning loop\n",
        "num_epochs = ## TODO define number of epochs\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = ## TODO the loss of output\n",
        "        ## TODO backward\n",
        "        optimizer.step()\n",
        "\n",
        "    print(## TODO print each epoch's loss)\n",
        "\n",
        "# Evaluate the fine-tuned model\n",
        "def generate_response(prompt):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=64).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(inputs.input_ids, max_length=64)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test the fine-tuned model\n",
        "prompt = ## TODO\n",
        "response = generate_response(prompt)\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Response: {response}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zgPWgZNyYodk",
      "metadata": {
        "id": "zgPWgZNyYodk"
      },
      "source": [
        "#### In-Context Learning(ICL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AnA1w0AuYnL1",
      "metadata": {
        "id": "AnA1w0AuYnL1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Initialize a lighter model for efficient in-context learning\n",
        "def initialize_model():\n",
        "    model_name = \"facebook/opt-125m\"  # A lightweight OPT model\n",
        "    tokenizer = ## TODO use pretrained tokenizer\n",
        "    model = ## TODO use pretrained model\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "opt_tokenizer, opt_model = initialize_model()\n",
        "\n",
        "def in_context_learning(prompt_examples, test_prompt, max_length=50, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Perform in-context learning by providing a few examples as part of the prompt.\n",
        "\n",
        "    Args:\n",
        "        prompt_examples (list of tuples): List of (input, output) examples.\n",
        "        test_prompt (str): The input for which the output needs to be predicted.\n",
        "        max_length (int): Maximum length of the generated text.\n",
        "        temperature (float): Sampling temperature for controlling randomness.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated output for the test_prompt.\n",
        "    \"\"\"\n",
        "    # Build the in-context prompt\n",
        "    context = \"\\n---\\n\".join([f\"Input: {inp}\\nOutput: {out}\" for inp, out in prompt_examples])\n",
        "    final_prompt = f\"{context}\\n---\\nInput: {test_prompt}\\nOutput:\"\n",
        "\n",
        "    # Tokenize and encode the prompt\n",
        "    inputs = opt_tokenizer(final_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "    # Generate the output\n",
        "    outputs = opt_model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=50,\n",
        "        pad_token_id=opt_tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode and return the generated text\n",
        "    generated_text = ## TODO\n",
        "\n",
        "    # Extract only the generated answer (remove context and test_prompt)\n",
        "    generated_output = generated_text[len(final_prompt):].strip()\n",
        "    return generated_output\n",
        "\n",
        "# Few-shot examples\n",
        "prompt_examples = [\n",
        " ## TODO\n",
        "]\n",
        "\n",
        "# Test input\n",
        "test_prompt = ## TODO\n",
        "\n",
        "# Generate the answer\n",
        "output = in_context_learning(prompt_examples, test_prompt, max_length=100, temperature=0.7)\n",
        "print(f\"Input: {test_prompt}\")\n",
        "print(f\"Generated Output: {output}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6f19462",
      "metadata": {
        "id": "d6f19462"
      },
      "source": [
        "## Prompting Paradigm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f97af9c",
      "metadata": {
        "id": "0f97af9c"
      },
      "source": [
        "\n",
        "Prompt engineering is critical for leveraging models like GPT-3. It involves:\n",
        "\n",
        "- Task-specific prompts to guide the model.\n",
        "- Advantages: Rapid prototyping, no parameter updates.\n",
        "- Disadvantages: Sensitivity to prompt design and structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dc76910",
      "metadata": {
        "id": "0dc76910"
      },
      "source": [
        "## Fine-tuning vs. In-context Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dba82266",
      "metadata": {
        "id": "dba82266"
      },
      "source": [
        "\n",
        "Comparison of techniques:\n",
        "\n",
        "- **Fine-tuning**: Adjusts model weights, often leading to better performance but requires more resources.\n",
        "- **In-context learning**: Provides task demonstrations as input without modifying the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe837506",
      "metadata": {
        "id": "fe837506"
      },
      "source": [
        "## Parameter-Efficient Fine-Tuning (PEFT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a39fe4d",
      "metadata": {
        "id": "3a39fe4d"
      },
      "source": [
        "\n",
        "PEFT methods include:\n",
        "\n",
        "1. **Adapters**: Add lightweight layers between transformer layers.\n",
        "2. **Prompt Tuning & Prefix Tuning**: Optimize prompts or prefixes without modifying the model.\n",
        "3. **LoRA (Low-Rank Adaptation)**: Fine-tune only low-rank updates to model parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iL20yJO7FY7P",
      "metadata": {
        "id": "iL20yJO7FY7P"
      },
      "source": [
        "#### Adapters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CFzM7aTeFalf",
      "metadata": {
        "id": "CFzM7aTeFalf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define a custom dataset for classification\n",
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            texts (list): List of input texts.\n",
        "            labels (list): List of corresponding labels.\n",
        "            tokenizer: Pre-trained tokenizer.\n",
        "            max_length (int): Maximum sequence length.\n",
        "        \"\"\"\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoded = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Define the Adapter class\n",
        "class Adapter(nn.Module):\n",
        "    def __init__(self, hidden_size, adapter_size):\n",
        "        super(Adapter, self).__init__()\n",
        "        self.down_proj = ## TODO\n",
        "        self.relu = ## TODO\n",
        "        self.up_proj = ## TODO\n",
        "\n",
        "    def forward(self, x):\n",
        "        down = self.down_proj(x)\n",
        "        activated = self.relu(down)\n",
        "        up = self.up_proj(activated)\n",
        "        return x + up  # Residual connection\n",
        "\n",
        "# Define the model class using BERT with Adapters\n",
        "class BERTWithAdapters(nn.Module):\n",
        "    def __init__(self, num_classes, adapter_size=64):\n",
        "        super(BERTWithAdapters, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.adapters = nn.ModuleList([\n",
        "            Adapter(self.bert.config.hidden_size, adapter_size)\n",
        "            for _ in range(self.bert.config.num_hidden_layers)\n",
        "        ])\n",
        "        self.dropout = ## TODO\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        hidden_states = outputs.hidden_states\n",
        "\n",
        "        # Apply adapters to each layer's hidden states\n",
        "        adapted_hidden_states = []\n",
        "        for i, adapter in enumerate(self.adapters):\n",
        "            adapted_hidden_states.append(adapter(hidden_states[i + 1]))\n",
        "\n",
        "        # Use the last adapter output for classification\n",
        "        pooled_output = adapted_hidden_states[-1][:, 0]  # Use [CLS] token representation\n",
        "        x = self.dropout(pooled_output)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BERTWithAdapters(num_classes=2)  # Binary classification (Positive/Negative)\n",
        "\n",
        "# Load GLUE SST-2 dataset\n",
        "data = load_dataset(\"glue\", \"sst2\")\n",
        "train_texts = ## TODO\n",
        "train_labels = ## TODO\n",
        "val_texts = ## TODO\n",
        "val_labels = ## TODO\n",
        "\n",
        "# Create dataset and dataloaders\n",
        "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length=64)\n",
        "val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length=64)\n",
        "train_dataloader = ## TODO with arbitary batch size.  ** Note : remember to shuffle\n",
        "val_dataloader = ## TODO\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = ## TODO use Adam\n",
        "loss_fn = ## TODO suitable loss function for binary classification ? :)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = ## TODO use the loss function you defined earlier\n",
        "        ## TODO Backward\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = ## TODO average loss of the train_dataloader\n",
        "    print ## TODO print each epoch's loss\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs, dim=1)\n",
        "        correct += ## TODO correct predictions\n",
        "        total += labels.size(0)\n",
        "\n",
        "accuracy = ## TODO calculate accuracy\n",
        "print ## TODO print accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12e7007b",
      "metadata": {
        "id": "12e7007b"
      },
      "source": [
        "## Intrinsic Dimensionality"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef764614",
      "metadata": {
        "id": "ef764614"
      },
      "source": [
        "\n",
        "Research shows that LLMs operate in a low intrinsic dimension, meaning effective fine-tuning can occur in smaller parameter spaces.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29e26eed",
      "metadata": {
        "id": "29e26eed"
      },
      "source": [
        "## LoRA (Low-Rank Adaptation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8227f17c",
      "metadata": {
        "id": "8227f17c"
      },
      "source": [
        "\n",
        "LoRA reduces the number of tunable parameters by:\n",
        "\n",
        "1. Keeping original weights fixed.\n",
        "2. Adding low-rank matrices to capture task-specific adaptations.\n",
        "\n",
        "### Results and Takeaways\n",
        "- Comparable performance to full fine-tuning with fewer parameters.\n",
        "- Sometimes even outperforms full fine-tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter-Efficient Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA)\n",
        "\n",
        "# Introduction to PEFT and LoRA\n",
        "## Parameter-Efficient Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA)\n",
        "\n",
        "### Why PEFT?\n",
        "Traditional fine-tuning of large language models requires updating all model parameters, which can be computationally expensive and memory-intensive. Parameter-Efficient Fine-Tuning (PEFT) techniques address this limitation by modifying only a small subset of parameters, reducing resource requirements significantly.\n",
        "\n",
        "### What is LoRA?\n",
        "LoRA (Low-Rank Adaptation) is a specific PEFT method that inserts low-rank matrices into the architecture of a pre-trained model, enabling efficient adaptation for downstream tasks. It is widely used for fine-tuning large language models like GPT and BERT without modifying their core parameters.\n",
        "\n",
        "In this section, we will implement LoRA for fine-tuning a pre-trained BART model on a text summarization task.\n"
      ],
      "metadata": {
        "id": "OljIoBdEBUTK"
      },
      "id": "OljIoBdEBUTK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install transformers peft datasets rouge-score --quiet"
      ],
      "metadata": {
        "id": "RNhTT3p9BUrk"
      },
      "id": "RNhTT3p9BUrk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from datasets import load_dataset\n",
        "# from datasets import load_metric\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "UMFrC6pTBelj"
      },
      "id": "UMFrC6pTBelj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = # TODO: load the cnn_dailymail dataset with 5000 training samples and train/test split"
      ],
      "metadata": {
        "id": "TmOKI58KBgql"
      },
      "id": "TmOKI58KBgql",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model and tokenizer\n",
        "model_name = # TODO: specify the pre-trained model name (e.g., \"facebook/bart-base\")\n",
        "tokenizer = # TODO\n",
        "model = # TODO: load the pre-trained model for Seq2SeqLM"
      ],
      "metadata": {
        "id": "XQlzwAoaBiR7"
      },
      "id": "XQlzwAoaBiR7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize dataset\n",
        "def preprocess_function(examples):\n",
        "    inputs = # TODO\n",
        "    model_inputs = # TODO: tokenize the inputs with a max length of 1024, truncation, and padding\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = # TODO\n",
        "\n",
        "    model_inputs[\"labels\"] = # TODO\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = # TODO"
      ],
      "metadata": {
        "id": "Tjw-J0D-Bm1M"
      },
      "id": "Tjw-J0D-Bm1M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA Configuration\n",
        "lora_config = # TODO: create a LoRA config with task type, rank, alpha, target modules, and dropout"
      ],
      "metadata": {
        "id": "f8AvDemYBnaU"
      },
      "id": "f8AvDemYBnaU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LoRA to the model\n",
        "peft_model = # TODO\n",
        "print(\"LoRA model created with PEFT.\")"
      ],
      "metadata": {
        "id": "hRk7n2fRBoik"
      },
      "id": "hRk7n2fRBoik",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = # TODO\n",
        "\n",
        "# Trainer setup\n",
        "trainer = # TODO"
      ],
      "metadata": {
        "id": "amhs2-ITBp85"
      },
      "id": "amhs2-ITBp85",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned LoRA model\n",
        "#TODO"
      ],
      "metadata": {
        "id": "9NtmqnsVBrmN"
      },
      "id": "9NtmqnsVBrmN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROUGE Metric\n",
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a popular metric for evaluating text summarization tasks. It measures the overlap between the generated summaries and reference summaries using metrics such as ROUGE-1, ROUGE-2, and ROUGE-L:\n",
        "\n",
        "- **ROUGE-1**: Measures overlap of unigrams (single words).\n",
        "- **ROUGE-2**: Measures overlap of bigrams (two consecutive words).\n",
        "- **ROUGE-L**: Considers the longest common subsequence (LCS).\n",
        "\n",
        "In this implementation, we use the `evaluate` library to compute ROUGE scores for the generated summaries compared to the ground truth.\n"
      ],
      "metadata": {
        "id": "HxodpmylBXLh"
      },
      "id": "HxodpmylBXLh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install evaluate library\n",
        "!pip install evaluate --quiet\n",
        "\n",
        "# evaluation metric\n",
        "rouge = # TODO"
      ],
      "metadata": {
        "id": "8ISR7YyQBXd_"
      },
      "id": "8ISR7YyQBXd_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions and evaluate\n",
        "def evaluate_model(trainer, dataset, tokenizer):\n",
        "    predictions, labels, _ = # TODO: use the trainer to generate predictions on the dataset\n",
        "    decoded_preds = # TODO: decode the predictions using the tokenizer\n",
        "    decoded_labels = # TODO: decode the labels using the tokenizer\n",
        "\n",
        "    result = # TODO: compute ROUGE scores using predictions and references\n",
        "    return decoded_preds, decoded_labels, {key: value for key, value in result.items()}\n",
        "\n",
        "preds, labels, results = # TODO\n",
        "print(\"ROUGE scores:\", results)"
      ],
      "metadata": {
        "id": "5YfkTE77Bufx"
      },
      "id": "5YfkTE77Bufx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictions vs Ground Truth\n",
        "\n",
        "Below are some examples of the model's predictions compared to the ground truth summaries:\n"
      ],
      "metadata": {
        "id": "0-1GjZ9XBYxI"
      },
      "id": "0-1GjZ9XBYxI"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):  # Display 3 samples\n",
        "    print(f\"\\n**Input Article {i+1}:**\\n\", # TODO: display the first 500 characters of the article, \"...\")\n",
        "    print(f\"\\n**Ground Truth Summary {i+1}:**\\n\", # TODO: display the ground truth summary)\n",
        "    print(f\"\\n**Model Prediction {i+1}:**\\n\", # TODO: display the model prediction)\n",
        "\n",
        "# Plot ROUGE scores\n",
        "# TODO: create a bar plot of the ROUGE scores\n",
        "plt.bar(results.keys(), results.values(), color='skyblue')\n",
        "plt.title(\"ROUGE Scores\")\n",
        "plt.ylabel(\"F1-Score\")\n",
        "plt.xlabel(\"Metric\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OZBcsgyUBZB0"
      },
      "id": "OZBcsgyUBZB0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After fine-tuning with LoRA, we can compare the performance against traditional fine-tuning:\n",
        "\n",
        "1. **Training Time**: LoRA reduces training time by only modifying specific parameters.\n",
        "2. **Memory Usage**: The low-rank matrices minimize memory consumption.\n",
        "3. **Performance Metrics**: Evaluate the ROUGE scores on the CNN/DailyMail test dataset.\n",
        "\n",
        "The bar chart above visualizes the ROUGE metrics, and the displayed predictions provide qualitative insights into the model's performance.\n"
      ],
      "metadata": {
        "id": "UgyVn87jBasC"
      },
      "id": "UgyVn87jBasC"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}