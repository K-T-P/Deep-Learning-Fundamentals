{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "88072146",
      "metadata": {
        "id": "88072146"
      },
      "source": [
        "# Language Models and Parameter-Efficient Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37e65237",
      "metadata": {
        "id": "37e65237"
      },
      "source": [
        "## Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54863f40",
      "metadata": {
        "id": "54863f40"
      },
      "source": [
        "\n",
        "Language models are fundamental to natural language processing. They come in three major categories:\n",
        "\n",
        "1. **Encoder-only models**: (e.g., BERT, RoBERTa, ELECTRA) - Best suited for understanding tasks such as classification and regression.\n",
        "2. **Encoder-decoder models**: (e.g., T5, BART) - Ideal for tasks like translation and summarization.\n",
        "3. **Decoder-only models**: (e.g., GPT-n models) - Primarily used for text generation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a11efa3",
      "metadata": {
        "id": "3a11efa3"
      },
      "source": [
        "## Autoregressive Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31880454",
      "metadata": {
        "id": "31880454"
      },
      "source": [
        "\n",
        "Autoregressive models predict the next token in a sequence based on previous tokens. This enables **conditional generation**, where outputs depend on the given prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "aXfsHcaJR_ll",
      "metadata": {
        "id": "aXfsHcaJR_ll"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NuNYRTyHQ2VM",
      "metadata": {
        "id": "NuNYRTyHQ2VM"
      },
      "outputs": [],
      "source": [
        "## Now let's use old - GPT2 for generating text\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "\n",
        "# Initialize GPT-2 model and tokenizer\n",
        "def initialize_gpt2():\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "gpt_tokenizer, gpt_model = initialize_gpt2()\n",
        "\n",
        "\n",
        "# Function to generate text using GPT-2\n",
        "def generate_text(prompt, max_length=50, temperature=1.0, top_k=50):\n",
        "    \"\"\"\n",
        "    Generate text using GPT-2 with customizable parameters.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The initial text to seed the model.\n",
        "        max_length (int): The maximum length of the generated text.\n",
        "        temperature (float): Sampling temperature. Higher values make output more random.\n",
        "        top_k (int): Limits sampling to the top-k most likely tokens.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated text.\n",
        "    \"\"\"\n",
        "    input_ids = gpt_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate text\n",
        "    outputs = gpt_model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "    return gpt_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "prompts = [\n",
        "    'Whose dialog is this? \"Say my name.\"'\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"Input Prompt: {prompt}\")\n",
        "    print(f\"Generated Text: {generate_text(prompt, max_length=100, temperature=0.7)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "682ef732",
      "metadata": {
        "id": "682ef732"
      },
      "source": [
        "## Large Language Models (LLMs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20f6d9e9",
      "metadata": {
        "id": "20f6d9e9"
      },
      "source": [
        "\n",
        "Large Language Models (LLMs) scale up the size and capacity of traditional language models. Key concepts include:\n",
        "\n",
        "- **Scale**: Models like GPT-3 have billions of parameters, leading to significant improvements in performance.\n",
        "- **Pre-training and Adaptation**: Pre-trained on massive datasets and later adapted to specific tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "264fa2d8",
      "metadata": {
        "id": "264fa2d8"
      },
      "source": [
        "[link text](https://)## Ways to Adapt to New Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3454d3d",
      "metadata": {
        "id": "e3454d3d"
      },
      "source": [
        "\n",
        "Methods to adapt pre-trained models include:\n",
        "\n",
        "1. **Zero-shot learning**: Use task descriptions as prompts without any training examples.\n",
        "2. **Few-shot learning**: Provide a small number of task-specific examples.\n",
        "3. **Lightweight Fine-tuning**: Modify only a subset of the model's parameters.\n",
        "4. **Fine-tuning for human-aligned models**: Align models with human preferences using fine-tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b1ff5a6",
      "metadata": {
        "id": "7b1ff5a6"
      },
      "source": [
        "## Zero-shot Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bb64bea",
      "metadata": {
        "id": "6bb64bea"
      },
      "source": [
        "\n",
        " Fine-tuning T5 on a multi-task dataset for zero-shot learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ck3JJrmzZCDe",
      "metadata": {
        "id": "Ck3JJrmzZCDe"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-yxyTiAgifkw",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-yxyTiAgifkw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "def zero_shot_question_answering(context, question, model_name=\"bigscience/T0_3B\"):\n",
        "    # Load the tokenizer and model from Hugging Face Hub\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "    # Prepare the input with the context and question\n",
        "    prompt = f\"context: {context} question: {question}\"\n",
        "\n",
        "    # Tokenize the prompt , with padding and truncation,\n",
        "    inputs = ## TODO\n",
        "\n",
        "    # Generate an answer from the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(inputs[\"input_ids\"], max_length=50, num_beams=5)\n",
        "\n",
        "    # Decode the generated output to get the answer\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Define the context and the question\n",
        "context = \"Albert Einstein was a theoretical physicist who developed the theory of relativity.\"\n",
        "question = \"Who developed the theory of relativity?\"\n",
        "\n",
        "# Perform question answering\n",
        "answer = zero_shot_question_answering(context, question)\n",
        "print(\"Answer:\", answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ada95b24",
      "metadata": {
        "id": "ada95b24"
      },
      "source": [
        "## Few-shot Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45063e96",
      "metadata": {
        "id": "45063e96"
      },
      "source": [
        "\n",
        "Few-shot learning strategies include:\n",
        "\n",
        "1. **Prompt-based fine-tuning**: Modify the prompt to improve performance.\n",
        "2. **In-context learning (ICL)**: Provide a few examples as part of the prompt for task demonstrations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZqbEPEXTfmrc",
      "metadata": {
        "id": "ZqbEPEXTfmrc"
      },
      "source": [
        "#### Prompt-based fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdjj0yw-fp0S",
      "metadata": {
        "id": "cdjj0yw-fp0S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from torch.optim import AdamW  # Use PyTorch's AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define a custom dataset for prompt-based fine-tuning\n",
        "class PromptDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prompt, response = self.data[idx]\n",
        "        encoded_input = self.tokenizer(\n",
        "            prompt,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        encoded_response = self.tokenizer(\n",
        "            response,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoded_input[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoded_input[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": encoded_response[\"input_ids\"].squeeze(0),\n",
        "        }\n",
        "\n",
        "# Load pre-trained model and tokenizer of t5-small\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Load SST-2 dataset\n",
        "raw_dataset = load_dataset(\"glue\", \"sst2\") ## use a dataset for sentiment analysis task like sst-2 or sst-5\n",
        "\n",
        "# Prepare data for fine-tuning\n",
        "processed_data = []\n",
        "for example in raw_dataset[\"train\"]:\n",
        "    sentence = example[\"sentence\"] #sentence of your each example in raw_dataset_train\n",
        "    label = \"positive\" if example[\"label\"] == 1 else \"negative\"  #define the label based on your dataset\n",
        "    prompt = f\"Sentiment analysis: {sentence} The sentiment is [MASK].\"\n",
        "    response = label\n",
        "    processed_data.append((prompt, response))\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = PromptDataset(processed_data, tokenizer, max_length=64)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True) # use the Dataloader with arbitary batch size.for example 16\n",
        "\n",
        "# Define optimizer and device\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)  # Use PyTorch's AdamW\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Fine-tuning loop\n",
        "num_epochs = 3 # define number of epochs\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = ## TODO the loss of output\n",
        "        ## TODO backward\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\") # print each epoch's loss)\n",
        "\n",
        "# Evaluate the fine-tuned model\n",
        "def generate_response(prompt):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=64).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(inputs.input_ids, max_length=64)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test the fine-tuned model\n",
        "prompt = \"Sentiment analysis: This movie was great! The sentiment is [MASK].\"\n",
        "response = generate_response(prompt)\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Response: {response}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zgPWgZNyYodk",
      "metadata": {
        "id": "zgPWgZNyYodk"
      },
      "source": [
        "#### In-Context Learning(ICL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AnA1w0AuYnL1",
      "metadata": {
        "id": "AnA1w0AuYnL1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "# Initialize a lighter model for efficient in-context learning\n",
        "def initialize_model():\n",
        "    model_name = \"facebook/opt-125m\"  # A lightweight OPT model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)  # use pretrained tokenizer\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)  # use pretrained model\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "opt_tokenizer, opt_model = initialize_model()\n",
        "\n",
        "\n",
        "def in_context_learning(prompt_examples, test_prompt, max_length=50, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Perform in-context learning by providing a few examples as part of the prompt.\n",
        "\n",
        "    Args:\n",
        "        prompt_examples (list of tuples): List of (input, output) examples.\n",
        "        test_prompt (str): The input for which the output needs to be predicted.\n",
        "        max_length (int): Maximum length of the generated text.\n",
        "        temperature (float): Sampling temperature for controlling randomness.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated output for the test_prompt.\n",
        "    \"\"\"\n",
        "    # Build the in-context prompt\n",
        "    context = \"\\n---\\n\".join(\n",
        "        [f\"Input: {inp}\\nOutput: {out}\" for inp, out in prompt_examples]\n",
        "    )\n",
        "    final_prompt = f\"{context}\\n---\\nInput: {test_prompt}\\nOutput:\"\n",
        "\n",
        "    # Tokenize and encode the prompt\n",
        "    inputs = opt_tokenizer(\n",
        "        final_prompt, return_tensors=\"pt\", padding=True, truncation=True\n",
        "    )\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "    # Generate the output\n",
        "    outputs = opt_model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=50,\n",
        "        pad_token_id=opt_tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    # Decode and return the generated text\n",
        "    generated_text = opt_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the generated answer (remove context and test_prompt)\n",
        "    generated_output = generated_text[len(final_prompt) :].strip()\n",
        "    return generated_output\n",
        "\n",
        "\n",
        "# Few-shot examples\n",
        "prompt_examples = [\n",
        "    (\"I love the sunny weather.\", \"positive\"),\n",
        "    (\"I am not happy with the service.\", \"negative\"),\n",
        "    (\"The food was excellent!\", \"positive\"),\n",
        "]\n",
        "\n",
        "# Test input\n",
        "test_prompt = \"The product quality is very good.\"\n",
        "\n",
        "# Generate the answer\n",
        "output = in_context_learning(\n",
        "    prompt_examples, test_prompt, max_length=100, temperature=0.7\n",
        ")\n",
        "print(f\"Input: {test_prompt}\")\n",
        "print(f\"Generated Output: {output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6f19462",
      "metadata": {
        "id": "d6f19462"
      },
      "source": [
        "## Prompting Paradigm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f97af9c",
      "metadata": {
        "id": "0f97af9c"
      },
      "source": [
        "\n",
        "Prompt engineering is critical for leveraging models like GPT-3. It involves:\n",
        "\n",
        "- Task-specific prompts to guide the model.\n",
        "- Advantages: Rapid prototyping, no parameter updates.\n",
        "- Disadvantages: Sensitivity to prompt design and structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dc76910",
      "metadata": {
        "id": "0dc76910"
      },
      "source": [
        "## Fine-tuning vs. In-context Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dba82266",
      "metadata": {
        "id": "dba82266"
      },
      "source": [
        "\n",
        "Comparison of techniques:\n",
        "\n",
        "- **Fine-tuning**: Adjusts model weights, often leading to better performance but requires more resources.\n",
        "- **In-context learning**: Provides task demonstrations as input without modifying the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe837506",
      "metadata": {
        "id": "fe837506"
      },
      "source": [
        "## Parameter-Efficient Fine-Tuning (PEFT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a39fe4d",
      "metadata": {
        "id": "3a39fe4d"
      },
      "source": [
        "\n",
        "PEFT methods include:\n",
        "\n",
        "1. **Adapters**: Add lightweight layers between transformer layers.\n",
        "2. **Prompt Tuning & Prefix Tuning**: Optimize prompts or prefixes without modifying the model.\n",
        "3. **LoRA (Low-Rank Adaptation)**: Fine-tune only low-rank updates to model parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iL20yJO7FY7P",
      "metadata": {
        "id": "iL20yJO7FY7P"
      },
      "source": [
        "#### Adapters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CFzM7aTeFalf",
      "metadata": {
        "id": "CFzM7aTeFalf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "# Define a custom dataset for classification\n",
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            texts (list): List of input texts.\n",
        "            labels (list): List of corresponding labels.\n",
        "            tokenizer: Pre-trained tokenizer.\n",
        "            max_length (int): Maximum sequence length.\n",
        "        \"\"\"\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoded = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "\n",
        "# Define the Adapter class\n",
        "class Adapter(nn.Module):\n",
        "    def __init__(self, hidden_size, adapter_size):\n",
        "        super(Adapter, self).__init__()\n",
        "        self.down_proj = nn.Linear(hidden_size, adapter_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.up_proj = nn.Linear(adapter_size, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        down = self.down_proj(x)\n",
        "        activated = self.relu(down)\n",
        "        up = self.up_proj(activated)\n",
        "        return x + up  # Residual connection\n",
        "\n",
        "\n",
        "# Define the model class using BERT with Adapters\n",
        "class BERTWithAdapters(nn.Module):\n",
        "    def __init__(self, num_classes, adapter_size=64):\n",
        "        super(BERTWithAdapters, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.adapters = nn.ModuleList(\n",
        "            [\n",
        "                Adapter(self.bert.config.hidden_size, adapter_size)\n",
        "                for _ in range(self.bert.config.num_hidden_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True,\n",
        "        )\n",
        "        hidden_states = outputs.hidden_states\n",
        "\n",
        "        # Apply adapters to each layer's hidden states\n",
        "        adapted_hidden_states = []\n",
        "        for i, adapter in enumerate(self.adapters):\n",
        "            adapted_hidden_states.append(adapter(hidden_states[i + 1]))\n",
        "\n",
        "        # Use the last adapter output for classification\n",
        "        pooled_output = adapted_hidden_states[-1][\n",
        "            :, 0\n",
        "        ]  # Use [CLS] token representation\n",
        "        x = self.dropout(pooled_output)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BERTWithAdapters(num_classes=2)  # Binary classification (Positive/Negative)\n",
        "\n",
        "# Load GLUE SST-2 dataset\n",
        "data = load_dataset(\"glue\", \"sst2\")\n",
        "train_texts = data[\"train\"][\"sentence\"]\n",
        "train_labels = data[\"train\"][\"label\"]\n",
        "val_texts = data[\"validation\"][\"sentence\"]\n",
        "val_labels = data[\"validation\"][\"label\"]\n",
        "\n",
        "# Create dataset and dataloaders\n",
        "train_dataset = TextClassificationDataset(\n",
        "    train_texts, train_labels, tokenizer, max_length=64\n",
        ")\n",
        "val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length=64)\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=16, shuffle=True\n",
        ")  # with arbitary batch size.  ** Note : remember to shuffle\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)  # use Adam\n",
        "loss_fn = nn.CrossEntropyLoss()  # suitable loss function for binary classification ? :)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_fn(outputs, labels)  # use the loss function you defined earlier\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(\n",
        "        train_dataloader\n",
        "    )  # average loss of the train_dataloader\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {avg_loss}\")\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs, dim=1)\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12e7007b",
      "metadata": {
        "id": "12e7007b"
      },
      "source": [
        "## Intrinsic Dimensionality"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef764614",
      "metadata": {
        "id": "ef764614"
      },
      "source": [
        "\n",
        "Research shows that LLMs operate in a low intrinsic dimension, meaning effective fine-tuning can occur in smaller parameter spaces.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29e26eed",
      "metadata": {
        "id": "29e26eed"
      },
      "source": [
        "## LoRA (Low-Rank Adaptation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8227f17c",
      "metadata": {
        "id": "8227f17c"
      },
      "source": [
        "\n",
        "LoRA reduces the number of tunable parameters by:\n",
        "\n",
        "1. Keeping original weights fixed.\n",
        "2. Adding low-rank matrices to capture task-specific adaptations.\n",
        "\n",
        "### Results and Takeaways\n",
        "- Comparable performance to full fine-tuning with fewer parameters.\n",
        "- Sometimes even outperforms full fine-tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OljIoBdEBUTK",
      "metadata": {
        "id": "OljIoBdEBUTK"
      },
      "source": [
        "## Parameter-Efficient Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA)\n",
        "\n",
        "# Introduction to PEFT and LoRA\n",
        "## Parameter-Efficient Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA)\n",
        "\n",
        "### Why PEFT?\n",
        "Traditional fine-tuning of large language models requires updating all model parameters, which can be computationally expensive and memory-intensive. Parameter-Efficient Fine-Tuning (PEFT) techniques address this limitation by modifying only a small subset of parameters, reducing resource requirements significantly.\n",
        "\n",
        "### What is LoRA?\n",
        "LoRA (Low-Rank Adaptation) is a specific PEFT method that inserts low-rank matrices into the architecture of a pre-trained model, enabling efficient adaptation for downstream tasks. It is widely used for fine-tuning large language models like GPT and BERT without modifying their core parameters.\n",
        "\n",
        "In this section, we will implement LoRA for fine-tuning a pre-trained BART model on a text summarization task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RNhTT3p9BUrk",
      "metadata": {
        "id": "RNhTT3p9BUrk"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install transformers peft datasets rouge-score --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UMFrC6pTBelj",
      "metadata": {
        "id": "UMFrC6pTBelj"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from datasets import load_dataset\n",
        "# from datasets import load_metric\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TmOKI58KBgql",
      "metadata": {
        "id": "TmOKI58KBgql"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\") # load the cnn_dailymail dataset with 5000 training samples and train/test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XQlzwAoaBiR7",
      "metadata": {
        "id": "XQlzwAoaBiR7"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"facebook/bart-base\" # specify the pre-trained model name (e.g., \"facebook/bart-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name) #load the pre-trained model for Seq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Tjw-J0D-Bm1M",
      "metadata": {
        "id": "Tjw-J0D-Bm1M"
      },
      "outputs": [],
      "source": [
        "# Tokenize dataset\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"article\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\") #tokenize the inputs with a max length of 1024, truncation, and padding\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"highlights\"], max_length=1024, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8AvDemYBnaU",
      "metadata": {
        "id": "f8AvDemYBnaU"
      },
      "outputs": [],
      "source": [
        "# LoRA Configuration\n",
        "lora_config = lora_config = LoraConfig(\n",
        "    task_type=\"seq2seq\",\n",
        "    rank=8,\n",
        "    alpha=16,\n",
        "    target_modules=[\"encoder\", \"decoder\"],\n",
        "    dropout=0.1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hRk7n2fRBoik",
      "metadata": {
        "id": "hRk7n2fRBoik"
      },
      "outputs": [],
      "source": [
        "# Apply LoRA to the model\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "print(\"LoRA model created with PEFT.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "amhs2-ITBp85",
      "metadata": {
        "id": "amhs2-ITBp85"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = # TODO\n",
        "\n",
        "# Trainer setup\n",
        "trainer = # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9NtmqnsVBrmN",
      "metadata": {
        "id": "9NtmqnsVBrmN"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned LoRA model\n",
        "#TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HxodpmylBXLh",
      "metadata": {
        "id": "HxodpmylBXLh"
      },
      "source": [
        "### ROUGE Metric\n",
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a popular metric for evaluating text summarization tasks. It measures the overlap between the generated summaries and reference summaries using metrics such as ROUGE-1, ROUGE-2, and ROUGE-L:\n",
        "\n",
        "- **ROUGE-1**: Measures overlap of unigrams (single words).\n",
        "- **ROUGE-2**: Measures overlap of bigrams (two consecutive words).\n",
        "- **ROUGE-L**: Considers the longest common subsequence (LCS).\n",
        "\n",
        "In this implementation, we use the `evaluate` library to compute ROUGE scores for the generated summaries compared to the ground truth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ISR7YyQBXd_",
      "metadata": {
        "id": "8ISR7YyQBXd_"
      },
      "outputs": [],
      "source": [
        "# Install evaluate library\n",
        "!pip install evaluate --quiet\n",
        "\n",
        "# evaluation metric\n",
        "rouge = # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5YfkTE77Bufx",
      "metadata": {
        "id": "5YfkTE77Bufx"
      },
      "outputs": [],
      "source": [
        "# Generate predictions and evaluate\n",
        "def evaluate_model(trainer, dataset, tokenizer):\n",
        "    predictions, labels, _ = trainer.predict(\n",
        "        dataset\n",
        "    )  # use the trainer to generate predictions on the dataset\n",
        "    decoded_preds = tokenizer.batch_decode(\n",
        "        predictions, skip_special_tokens=True\n",
        "    )  # decode the predictions using the tokenizer\n",
        "    decoded_labels = tokenizer.batch_decode(\n",
        "        labels, skip_special_tokens=True\n",
        "    )  # decode the labels using the tokenizer\n",
        "\n",
        "    result = rouge.compute(\n",
        "        predictions=decoded_preds, references=decoded_labels\n",
        "    )  # compute ROUGE scores using predictions and references\n",
        "    return decoded_preds, decoded_labels, {key: value for key, value in result.items()}\n",
        "\n",
        "\n",
        "preds, labels, results = evaluate_model(\n",
        "    trainer, tokenized_datasets[\"validation\"], tokenizer\n",
        ")\n",
        "print(\"ROUGE scores:\", results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0-1GjZ9XBYxI",
      "metadata": {
        "id": "0-1GjZ9XBYxI"
      },
      "source": [
        "### Predictions vs Ground Truth\n",
        "\n",
        "Below are some examples of the model's predictions compared to the ground truth summaries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OZBcsgyUBZB0",
      "metadata": {
        "id": "OZBcsgyUBZB0"
      },
      "outputs": [],
      "source": [
        "for i in range(3):  # Display 3 samples\n",
        "    print(f\"\\n**Input Article {i+1}:**\\n\", # TODO: display the first 500 characters of the article, \"...\")\n",
        "    print(f\"\\n**Ground Truth Summary {i+1}:**\\n\", # TODO: display the ground truth summary)\n",
        "    print(f\"\\n**Model Prediction {i+1}:**\\n\", # TODO: display the model prediction)\n",
        "\n",
        "# Plot ROUGE scores\n",
        "# TODO: create a bar plot of the ROUGE scores\n",
        "plt.bar(results.keys(), results.values(), color='skyblue')\n",
        "plt.title(\"ROUGE Scores\")\n",
        "plt.ylabel(\"F1-Score\")\n",
        "plt.xlabel(\"Metric\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UgyVn87jBasC",
      "metadata": {
        "id": "UgyVn87jBasC"
      },
      "source": [
        "After fine-tuning with LoRA, we can compare the performance against traditional fine-tuning:\n",
        "\n",
        "1. **Training Time**: LoRA reduces training time by only modifying specific parameters.\n",
        "2. **Memory Usage**: The low-rank matrices minimize memory consumption.\n",
        "3. **Performance Metrics**: Evaluate the ROUGE scores on the CNN/DailyMail test dataset.\n",
        "\n",
        "The bar chart above visualizes the ROUGE metrics, and the displayed predictions provide qualitative insights into the model's performance.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
